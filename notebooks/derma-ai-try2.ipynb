{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ğŸ“¦ Install\n!pip install -q -U albumentations\n\n# ğŸ“š Imports\nimport numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport random\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom tensorflow.keras.applications import DenseNet201\nfrom tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport albumentations as A\nfrom albumentations.core.composition import OneOf\nfrom tensorflow.keras.utils import to_categorical","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T10:53:35.570509Z","iopub.execute_input":"2025-05-15T10:53:35.570730Z","iopub.status.idle":"2025-05-15T10:54:13.578141Z","shell.execute_reply.started":"2025-05-15T10:53:35.570709Z","shell.execute_reply":"2025-05-15T10:54:13.577583Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"},{"name":"stderr","text":"2025-05-15 10:53:48.157063: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747306428.640224      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747306428.758470      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ğŸ’¾ Dataset Paths\nCSV_PATH = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\"\nIMAGE_DIR_1 = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1\"\nIMAGE_DIR_2 = \"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T10:54:13.579390Z","iopub.execute_input":"2025-05-15T10:54:13.579880Z","iopub.status.idle":"2025-05-15T10:54:13.583623Z","shell.execute_reply.started":"2025-05-15T10:54:13.579857Z","shell.execute_reply":"2025-05-15T10:54:13.582991Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ğŸ§¹ Load and preprocess data\ndf = pd.read_csv(CSV_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T10:54:13.584363Z","iopub.execute_input":"2025-05-15T10:54:13.584578Z","iopub.status.idle":"2025-05-15T10:54:13.688298Z","shell.execute_reply.started":"2025-05-15T10:54:13.584561Z","shell.execute_reply":"2025-05-15T10:54:13.687765Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def get_image_path(image_id):\n    path1 = os.path.join(IMAGE_DIR_1, f\"{image_id}.jpg\")\n    path2 = os.path.join(IMAGE_DIR_2, f\"{image_id}.jpg\")\n    return path1 if os.path.exists(path1) else path2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T10:54:13.688966Z","iopub.execute_input":"2025-05-15T10:54:13.689164Z","iopub.status.idle":"2025-05-15T10:54:13.693085Z","shell.execute_reply.started":"2025-05-15T10:54:13.689147Z","shell.execute_reply":"2025-05-15T10:54:13.692402Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df['image_path'] = df['image_id'].apply(get_image_path)\ndf['label'] = LabelEncoder().fit_transform(df['dx'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T10:54:13.694790Z","iopub.execute_input":"2025-05-15T10:54:13.695017Z","iopub.status.idle":"2025-05-15T10:54:35.600536Z","shell.execute_reply.started":"2025-05-15T10:54:13.695001Z","shell.execute_reply":"2025-05-15T10:54:35.599673Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# âœ… Parameters\nIMG_SIZE = 224\nBATCH_SIZE = 32\nEPOCHS = 30\nNUM_CLASSES = df['label'].nunique()\nSEED = 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T10:54:35.601461Z","iopub.execute_input":"2025-05-15T10:54:35.601761Z","iopub.status.idle":"2025-05-15T10:54:35.611406Z","shell.execute_reply.started":"2025-05-15T10:54:35.601736Z","shell.execute_reply":"2025-05-15T10:54:35.610558Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ğŸ§ª Albumentations augmentations\nfrom albumentations import (\n    Resize, HorizontalFlip, RandomBrightnessContrast, HueSaturationValue,\n    Affine, Normalize\n)\nfrom albumentations.augmentations.dropout.coarse_dropout import CoarseDropout\n\ntrain_transform = A.Compose([\n    Resize(IMG_SIZE, IMG_SIZE),\n    HorizontalFlip(p=0.5),\n    RandomBrightnessContrast(p=0.5),\n    Affine(rotate=(-15, 15), scale=(0.95, 1.05), translate_percent=(0.0, 0.05), p=0.5),\n    HueSaturationValue(p=0.4),\n    CoarseDropout(max_holes=8, max_height=16, max_width=16, fill_value=0, p=0.3),\n    Normalize()\n])\n\n\nval_transform = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),\n    A.Normalize()\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T10:54:35.612622Z","iopub.execute_input":"2025-05-15T10:54:35.612906Z","iopub.status.idle":"2025-05-15T10:54:35.673831Z","shell.execute_reply.started":"2025-05-15T10:54:35.612880Z","shell.execute_reply":"2025-05-15T10:54:35.673156Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/286847742.py:14: UserWarning: Argument(s) 'max_holes, max_height, max_width, fill_value' are not valid for transform CoarseDropout\n  CoarseDropout(max_holes=8, max_height=16, max_width=16, fill_value=0, p=0.3),\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ğŸ§  Image loader\ndef load_image(path, transform=None):\n    image = cv2.imread(path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    if transform:\n        image = transform(image=image)[\"image\"]\n    return image.astype(np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T10:54:35.674609Z","iopub.execute_input":"2025-05-15T10:54:35.674843Z","iopub.status.idle":"2025-05-15T10:54:35.693759Z","shell.execute_reply.started":"2025-05-15T10:54:35.674816Z","shell.execute_reply":"2025-05-15T10:54:35.693199Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ğŸ“¦ Dataset generator\nclass SkinDataset(tf.keras.utils.Sequence):\n    def __init__(self, image_paths, labels, batch_size, transform):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.batch_size = batch_size\n        self.transform = transform\n        self.indices = np.arange(len(self.image_paths))\n\n    def __len__(self):\n        return len(self.image_paths) // self.batch_size\n\n    def __getitem__(self, idx):\n        batch_paths = self.image_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n        images = [load_image(p, self.transform) for p in batch_paths]\n        return np.stack(images), to_categorical(batch_labels, num_classes=NUM_CLASSES)\n\n    def on_epoch_end(self):\n        np.random.shuffle(self.indices)\n        self.image_paths = [self.image_paths[i] for i in self.indices]\n        self.labels = [self.labels[i] for i in self.indices]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T10:54:35.694415Z","iopub.execute_input":"2025-05-15T10:54:35.694640Z","iopub.status.idle":"2025-05-15T10:54:35.712069Z","shell.execute_reply.started":"2025-05-15T10:54:35.694625Z","shell.execute_reply":"2025-05-15T10:54:35.711546Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# ğŸ”¥ Focal Loss\ndef focal_loss(gamma=2., alpha=0.25):\n    def loss_fn(y_true, y_pred):\n        epsilon = 1e-9\n        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        weight = alpha * tf.math.pow(1 - y_pred, gamma)\n        return tf.reduce_mean(weight * cross_entropy)\n    return loss_fn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T10:54:35.712726Z","iopub.execute_input":"2025-05-15T10:54:35.712918Z","iopub.status.idle":"2025-05-15T10:54:35.729980Z","shell.execute_reply.started":"2025-05-15T10:54:35.712903Z","shell.execute_reply":"2025-05-15T10:54:35.729462Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ğŸ—ï¸ Build Model\ndef build_model():\n    base = DenseNet201(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n    base.trainable = False\n\n    x = base.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dropout(0.5)(x)\n    output = Dense(NUM_CLASSES, activation='softmax')(x)\n\n    model = Model(inputs=base.input, outputs=output)\n    model.compile(optimizer='adam',\n                  loss=focal_loss(gamma=2.0, alpha=0.25),\n                  metrics=['accuracy'])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T10:54:35.730638Z","iopub.execute_input":"2025-05-15T10:54:35.730895Z","iopub.status.idle":"2025-05-15T10:54:35.753513Z","shell.execute_reply.started":"2025-05-15T10:54:35.730870Z","shell.execute_reply":"2025-05-15T10:54:35.752997Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# ğŸ” Stratified K-Fold Training\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(df, df['label'])):\n    print(f\"\\nğŸ“‚ Training Fold {fold + 1}\")\n\n    train_paths = df.iloc[train_idx]['image_path'].values\n    val_paths = df.iloc[val_idx]['image_path'].values\n    train_labels = df.iloc[train_idx]['label'].values\n    val_labels = df.iloc[val_idx]['label'].values\n\n    train_gen = SkinDataset(train_paths, train_labels, BATCH_SIZE, transform=train_transform)\n    val_gen = SkinDataset(val_paths, val_labels, BATCH_SIZE, transform=val_transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T10:54:35.754135Z","iopub.execute_input":"2025-05-15T10:54:35.754321Z","iopub.status.idle":"2025-05-15T10:54:35.798026Z","shell.execute_reply.started":"2025-05-15T10:54:35.754301Z","shell.execute_reply":"2025-05-15T10:54:35.797474Z"}},"outputs":[{"name":"stdout","text":"\nğŸ“‚ Training Fold 1\n\nğŸ“‚ Training Fold 2\n\nğŸ“‚ Training Fold 3\n\nğŸ“‚ Training Fold 4\n\nğŸ“‚ Training Fold 5\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":" model = build_model()\n\n callbacks = [\n        EarlyStopping(patience=5, restore_best_weights=True),\n        ReduceLROnPlateau(patience=3, factor=0.3, verbose=1)\n    ]\n\n model.fit(train_gen,\n              validation_data=val_gen,\n              epochs=EPOCHS,\n              callbacks=callbacks,\n              verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T10:54:35.798723Z","iopub.execute_input":"2025-05-15T10:54:35.799002Z","iopub.status.idle":"2025-05-15T11:32:14.593103Z","shell.execute_reply.started":"2025-05-15T10:54:35.798921Z","shell.execute_reply":"2025-05-15T11:32:14.592406Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1747306476.261095      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1747306476.261863      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m74836368/74836368\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1747306515.520651     104 service.cc:148] XLA service 0x7a3d600e9dd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1747306515.522284     104 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1747306515.522306     104 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1747306520.129772     104 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  1/250\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m3:54:19\u001b[0m 56s/step - accuracy: 0.0625 - loss: 0.0784","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1747306540.812274     104 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 1s/step - accuracy: 0.5690 - loss: 0.0358 - val_accuracy: 0.7293 - val_loss: 0.0158 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 436ms/step - accuracy: 0.6884 - loss: 0.0205 - val_accuracy: 0.7455 - val_loss: 0.0130 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 455ms/step - accuracy: 0.6898 - loss: 0.0183 - val_accuracy: 0.7692 - val_loss: 0.0122 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 429ms/step - accuracy: 0.7033 - loss: 0.0177 - val_accuracy: 0.7737 - val_loss: 0.0121 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 437ms/step - accuracy: 0.7019 - loss: 0.0177 - val_accuracy: 0.7661 - val_loss: 0.0123 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 428ms/step - accuracy: 0.7096 - loss: 0.0163 - val_accuracy: 0.7727 - val_loss: 0.0120 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 422ms/step - accuracy: 0.7043 - loss: 0.0168 - val_accuracy: 0.7777 - val_loss: 0.0118 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 422ms/step - accuracy: 0.7118 - loss: 0.0172 - val_accuracy: 0.7767 - val_loss: 0.0118 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 420ms/step - accuracy: 0.7139 - loss: 0.0160 - val_accuracy: 0.7772 - val_loss: 0.0114 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 428ms/step - accuracy: 0.7040 - loss: 0.0165 - val_accuracy: 0.7717 - val_loss: 0.0117 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 429ms/step - accuracy: 0.7257 - loss: 0.0161 - val_accuracy: 0.7782 - val_loss: 0.0119 - learning_rate: 0.0010\nEpoch 12/30\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step - accuracy: 0.6997 - loss: 0.0169\nEpoch 12: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 432ms/step - accuracy: 0.6997 - loss: 0.0169 - val_accuracy: 0.7742 - val_loss: 0.0116 - learning_rate: 0.0010\nEpoch 13/30\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 432ms/step - accuracy: 0.7271 - loss: 0.0152 - val_accuracy: 0.7833 - val_loss: 0.0112 - learning_rate: 3.0000e-04\nEpoch 14/30\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 422ms/step - accuracy: 0.7230 - loss: 0.0150 - val_accuracy: 0.7823 - val_loss: 0.0109 - learning_rate: 3.0000e-04\nEpoch 15/30\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 428ms/step - accuracy: 0.7302 - loss: 0.0146 - val_accuracy: 0.7797 - val_loss: 0.0113 - learning_rate: 3.0000e-04\nEpoch 16/30\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 424ms/step - accuracy: 0.7330 - loss: 0.0149 - val_accuracy: 0.7782 - val_loss: 0.0112 - learning_rate: 3.0000e-04\nEpoch 17/30\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363ms/step - accuracy: 0.7322 - loss: 0.0149\nEpoch 17: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 440ms/step - accuracy: 0.7322 - loss: 0.0149 - val_accuracy: 0.7838 - val_loss: 0.0111 - learning_rate: 3.0000e-04\nEpoch 18/30\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 454ms/step - accuracy: 0.7258 - loss: 0.0145 - val_accuracy: 0.7863 - val_loss: 0.0111 - learning_rate: 9.0000e-05\nEpoch 19/30\n\u001b[1m250/250\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 428ms/step - accuracy: 0.7363 - loss: 0.0148 - val_accuracy: 0.7843 - val_loss: 0.0110 - learning_rate: 9.0000e-05\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7a3ddab51110>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":" model.save(f\"densenet201_fold{fold+1}.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:32:14.595380Z","iopub.execute_input":"2025-05-15T11:32:14.595632Z","iopub.status.idle":"2025-05-15T11:32:15.734785Z","shell.execute_reply.started":"2025-05-15T11:32:14.595614Z","shell.execute_reply":"2025-05-15T11:32:15.734185Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nimport cv2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:32:15.735913Z","iopub.execute_input":"2025-05-15T11:32:15.736095Z","iopub.status.idle":"2025-05-15T11:32:15.739847Z","shell.execute_reply.started":"2025-05-15T11:32:15.736081Z","shell.execute_reply":"2025-05-15T11:32:15.739119Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    grad_model = tf.keras.models.Model(\n        [model.inputs], \n        [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(predictions[0])\n        class_output = predictions[:, pred_index]\n\n    grads = tape.gradient(class_output, conv_outputs)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    conv_outputs = conv_outputs[0]\n    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:32:15.740568Z","iopub.execute_input":"2025-05-15T11:32:15.740737Z","iopub.status.idle":"2025-05-15T11:32:15.764223Z","shell.execute_reply.started":"2025-05-15T11:32:15.740720Z","shell.execute_reply":"2025-05-15T11:32:15.763547Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def overlay_heatmap(original_img, heatmap, alpha=0.4, colormap=cv2.COLORMAP_JET):\n    heatmap = cv2.resize(heatmap, (original_img.shape[1], original_img.shape[0]))\n    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap), colormap)\n    overlayed_img = cv2.addWeighted(original_img, 1 - alpha, heatmap_colored, alpha, 0)\n    return overlayed_img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:32:15.764971Z","iopub.execute_input":"2025-05-15T11:32:15.765204Z","iopub.status.idle":"2025-05-15T11:32:15.792196Z","shell.execute_reply.started":"2025-05-15T11:32:15.765183Z","shell.execute_reply":"2025-05-15T11:32:15.791537Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def predict_and_visualize(image_path, model, label_map, last_conv_layer='conv5_block32_concat'):\n    img = cv2.imread(image_path)\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img_resized = cv2.resize(img_rgb, (224, 224)) / 255.0\n    img_input = np.expand_dims(img_resized, axis=0)\n\n    # Prediction\n    preds = model.predict(img_input)\n    pred_class = np.argmax(preds[0])\n    confidence = np.max(preds[0])\n\n    # Grad-CAM\n    heatmap = make_gradcam_heatmap(img_input, model, last_conv_layer)\n    cam_image = overlay_heatmap(img_rgb, heatmap)\n\n    # Display\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.title(\"Original Image\")\n    plt.imshow(img_rgb)\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.title(f\"Grad-CAM: {label_map[pred_class]} ({confidence:.2f})\")\n    plt.imshow(cam_image)\n    plt.axis('off')\n    plt.show()\n\n    return label_map[pred_class], confidence\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:32:15.792918Z","iopub.execute_input":"2025-05-15T11:32:15.793606Z","iopub.status.idle":"2025-05-15T11:32:15.809906Z","shell.execute_reply.started":"2025-05-15T11:32:15.793585Z","shell.execute_reply":"2025-05-15T11:32:15.809345Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Example label map\nlabel_map = {\n    0: \"Actinic keratoses\",\n    1: \"Basal cell carcinoma\",\n    2: \"Benign keratosis-like lesions\",\n    3: \"Dermatofibroma\",\n    4: \"Melanoma\",\n    5: \"Melanocytic nevi\",\n    6: \"Vascular lesions\"\n}\n\n# Load model (update path if needed)\nmodel = load_model(\"/kaggle/working/densenet201_fold5.h5\", compile=False)\n\n# Predict & visualize\npredicted_label, confidence = predict_and_visualize(\n    \"/kaggle/input/ham10000-images-part-1/ISIC_0027419.jpg\",  # update with your image file name\n    model,\n    label_map,\n    conv_layer=\"top_activation\"\n  # DenseNet201 final conv layer\n)\n\nprint(f\"Prediction: {predicted_label}, Confidence: {confidence:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T11:33:55.066865Z","iopub.execute_input":"2025-05-15T11:33:55.067384Z","iopub.status.idle":"2025-05-15T11:33:57.953192Z","shell.execute_reply.started":"2025-05-15T11:33:55.067360Z","shell.execute_reply":"2025-05-15T11:33:57.952226Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/327024028.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Predict & visualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m predicted_label, confidence = predict_and_visualize(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;34m\"/kaggle/input/ham10000-images-part-1/ISIC_0027419.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# update with your image file name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: predict_and_visualize() got an unexpected keyword argument 'conv_layer'"],"ename":"TypeError","evalue":"predict_and_visualize() got an unexpected keyword argument 'conv_layer'","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}